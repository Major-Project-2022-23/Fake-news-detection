{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import urllib\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import tensorflow as tf\n",
    "import torch.nn as nn\n",
    "import texthero as hero\n",
    "from urllib.parse import urlsplit\n",
    "import requests\n",
    "import torch\n",
    "from bs4 import BeautifulSoup\n",
    "from requests import get\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import XLNetTokenizer, XLNetForSequenceClassification\n",
    "from huggingface_hub import from_pretrained_keras\n",
    "import spacy_sentence_bert\n",
    "import csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from summarizer import Summarizer,TransformerSummarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO\n",
    "1. News section search - Done\n",
    "2. Abstarctive summary\n",
    "3. Specify cap on number of links\n",
    "4. Search Keywords\n",
    "5. Tune similarity parameter n Model\n",
    "6. Use summary or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search Google News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from GoogleNews import GoogleNews\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "googlenews = GoogleNews(start='12/31/2019',end='01/01/2023', lang='en')\n",
    "googlenews.set_encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links_from_google_news(claim, top=10):\n",
    "    googlenews.search(claim)\n",
    "    \n",
    "    results = googlenews.results()\n",
    "    googlenews.clear()\n",
    "    \n",
    "#     results = [x for x in results if isinstance(x['datetime'], datetime)]\n",
    "    \n",
    "#     def func(ele):\n",
    "#         return str(ele['datetime'])\n",
    "    \n",
    "#     results.sort(key = func, reverse = True)\n",
    "    links = [x['link'] for x in results]\n",
    "    \n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Evidences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy_sentence_bert.load_model('en_nli_roberta_base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences_from_link(link, text, top=10):\n",
    "    request = requests.get(link, verify=False, timeout=20)\n",
    "    Soup = BeautifulSoup(request.text, 'lxml')\n",
    "    \n",
    "    heading_tags = ['p']\n",
    "\n",
    "\n",
    "    results = []\n",
    "    used = []\n",
    "\n",
    "    for tags in Soup.find_all(heading_tags):\n",
    "        if 'h' in tags.name:\n",
    "            tokens = tags.text.strip().split()\n",
    "            if len(tokens) > 8:\n",
    "                if tags.text.strip() not in used:\n",
    "                    used.append(tags.text.strip())\n",
    "                    results.append([tags.name, tags.text.strip()])\n",
    "        else:\n",
    "            tokens = tags.text.strip().split()\n",
    "            if len(tokens) > 8:\n",
    "                if tags.text.strip() not in used:\n",
    "                    used.append(tags.text.strip())\n",
    "                    results.append([tags.name, tags.text.strip()])\n",
    "    doc1 = nlp(text)\n",
    "    sim = []\n",
    "    for r in results:\n",
    "        sim.append(doc1.similarity(nlp(r[1])))\n",
    "    zipped = zip(sim, results)\n",
    "    zipped = sorted(zipped, reverse=True)\n",
    "    high_conf = [a for s, a in zipped if s >= 0.6]\n",
    "\n",
    "    return high_conf[:top], request.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_evidences(text, links):\n",
    "    new_links = []\n",
    "    for link in links:\n",
    "        conf, lin = get_sentences_from_link(link, text)\n",
    "        new_links.append([lin, conf])\n",
    "    return new_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatinating Evidences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_evidences(claim, links):\n",
    "    summ = []\n",
    "    for link in links:\n",
    "        if type(link[1]) == list:\n",
    "            for text in link[1]:\n",
    "                if type(link[1]) == list:\n",
    "                    summ.append(text[1])\n",
    "                else:\n",
    "                    summ.append(text)\n",
    "        elif type(link[1]) == str:\n",
    "            summ.append(link[1])\n",
    "\n",
    "    urls = re.findall(r'https?:\\/+\\/+t+\\.+co+\\/+\\S*', claim)\n",
    "    \n",
    "    for li in urls:\n",
    "        claim = claim.replace(li, '')\n",
    "    claim = claim.strip()\n",
    "\n",
    "    if summ:\n",
    "        summary = (claim, ' '.join(summ).replace('\\n', '').replace('\\t', ''))\n",
    "    else:\n",
    "        summary = ('', '')\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extractive Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarizer = Summarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_extractive_summary(evidence):\n",
    "#     return ''.join(summarizer(evidence, min_length=60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstractive Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abstractive_summarizer = pipeline(\"summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abstractive_summarizer = pipeline(\"summarization\", model=\"t5-base\", tokenizer=\"t5-base\", framework=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_abstractive_summary(evidence):\n",
    "#     return abstractive_summarizer(evidence, min_length=5, do_sample=False)[0]['summary_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Based Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSemanticDataGenerator(tf.keras.utils.Sequence):\n",
    "    \"\"\"Generates batches of data.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        sentence_pairs,\n",
    "        labels,\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        include_targets=True,\n",
    "    ):\n",
    "        self.sentence_pairs = sentence_pairs\n",
    "        self.labels = labels\n",
    "        self.shuffle = shuffle\n",
    "        self.batch_size = batch_size\n",
    "        self.include_targets = include_targets\n",
    "        # Load our BERT Tokenizer to encode the text.\n",
    "        # We will use base-base-uncased pretrained model.\n",
    "        self.tokenizer = transformers.BertTokenizer.from_pretrained(\n",
    "            \"bert-base-uncased\", do_lower_case=True\n",
    "        )\n",
    "        self.indexes = np.arange(len(self.sentence_pairs))\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        # Denotes the number of batches per epoch.\n",
    "        return len(self.sentence_pairs) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieves the batch of index.\n",
    "        indexes = self.indexes[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
    "        sentence_pairs = self.sentence_pairs[indexes]\n",
    "\n",
    "        # With BERT tokenizer's batch_encode_plus batch of both the sentences are\n",
    "        # encoded together and separated by [SEP] token.\n",
    "        encoded = self.tokenizer.batch_encode_plus(\n",
    "            sentence_pairs.tolist(),\n",
    "            add_special_tokens=True,\n",
    "            max_length=128,\n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=True,\n",
    "            pad_to_max_length=True,\n",
    "            return_tensors=\"tf\",\n",
    "        )\n",
    "\n",
    "        # Convert batch of encoded features to numpy array.\n",
    "        input_ids = np.array(encoded[\"input_ids\"], dtype=\"int32\")\n",
    "        attention_masks = np.array(encoded[\"attention_mask\"], dtype=\"int32\")\n",
    "        token_type_ids = np.array(encoded[\"token_type_ids\"], dtype=\"int32\")\n",
    "\n",
    "        # Set to true if data generator is used for training/validation.\n",
    "        if self.include_targets:\n",
    "            labels = np.array(self.labels[indexes], dtype=\"int32\")\n",
    "            return [input_ids, attention_masks, token_type_ids], labels\n",
    "        else:\n",
    "            return [input_ids, attention_masks, token_type_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = from_pretrained_keras(\"keras-io/bert-semantic-similarity\")\n",
    "labels = [\"contradiction\", \"entailment\", \"neutral\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sentence1, sentence2):\n",
    "    sentence_pairs = np.array([[str(sentence1), str(sentence2)]])\n",
    "    test_data = BertSemanticDataGenerator(\n",
    "        sentence_pairs, labels=None, batch_size=1, shuffle=False, include_targets=False,\n",
    "    )\n",
    "    probs = model.predict(test_data[0])[0]\n",
    "    \n",
    "    labels_probs = {labels[i]: float(probs[i]) for i, _ in enumerate(labels)}\n",
    "    return labels_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fake News Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fake_news_detection(claim):\n",
    "#     start = timer()\n",
    "    links = get_links_from_google_news(claim)\n",
    "#     print(\"links ----- \")\n",
    "#     print(links)\n",
    "    evidence_list = scrap_evidences(claim, links[:10])\n",
    "#     print(\"Evidence List ----- \")\n",
    "#     print(evidence_list)\n",
    "    evidence = concatenate_evidences(claim, evidence_list)\n",
    "#     print(\"Concatenated Evidence ----- \")\n",
    "#     print(evidence[1])\n",
    "#     print(\"Results with Concatenated Evidence\")\n",
    "    k = predict(claim, evidence[1])\n",
    "    print(k)\n",
    "#     extractive_summary = generate_extractive_summary(evidence[1])\n",
    "#     print(\"Extractive Summary ----- \")\n",
    "#     print(extractive_summary)\n",
    "#     print(\"Results with Extractive Summary\")\n",
    "#     print(predict(claim, extractive_summary))\n",
    "#     abstractive_summary = generate_abstractive_summary(evidence[1])\n",
    "#     print(\"Abstractive Summary ----- \")\n",
    "#     print(abstractive_summary)\n",
    "#     print(\"Results with Abstractive Summary\")\n",
    "#     print(predict(claim, abstractive_summary))\n",
    "#     end = timer()\n",
    "#     print(end - start)\n",
    "#     return evidence[1]\n",
    "    return (k,  evidence[1])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# claim = \"Times Of India Shares Fake Cropped Bungee Jump Fail Video\"\n",
    "# # claim = \"People from LGBTQ Community can marry in india\"\n",
    "# v = fake_news_detection(claim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_abstractive_summary(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('IFND_dataset.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data.sample(frac = 1)\n",
    "# data = data.reset_index()\n",
    "# data = data.drop(['index'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## VERY SENSITIVE PIECE OF CODE\n",
    "# with open(filename, 'w') as csvfile:\n",
    "#     csvwriter = csv.writer(csvfile)\n",
    "#     csvwriter.writerow(['id', 'contradiction', 'entailment', 'neutral', 'evidence'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filename, 'a') as csvfile: \n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    for i in range(77, 200):\n",
    "        print(i)\n",
    "        try:\n",
    "            v = fake_news_detection(data.iloc[i]['Statement'])\n",
    "            csvwriter.writerow([data.iloc[i]['id'], v[0]['contradiction'], v[0]['entailment'], v[0]['neutral'], v[1]])\n",
    "        except:\n",
    "            print(\"failed at \" + str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
